{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data = np.loadtxt('data.csv', delimiter=',', dtype=float)\n",
    "labels = data[:, 0]\n",
    "features = preprocessing.minmax_scale(data[:, 1:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels.ravel(), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무\n",
    "* random_state = 2022 으로 설정\n",
    "* 변수명은 dt_clf 로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 부분 코드 작성\n",
    "# decision tree\n",
    "dt_clf = DecisionTreeClassifier(random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=2022)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "dt_pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score : 0.8415841584158416\n",
      "precision_score : 0.7843137254901961\n",
      "recall_score : 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(f'accuracy_score : {accuracy_score(dt_pred, y_test)}')\n",
    "print(f'precision_score : {precision_score(dt_pred, y_test)}')\n",
    "print(f'recall_score : {recall_score(dt_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트\n",
    "* random_state = 2022\n",
    "* 변수명 rf_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_clf = RandomForestClassifier(random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=2022)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "rf_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score : 0.8910891089108911\n",
      "precision_score : 0.8431372549019608\n",
      "recall_score : 0.9347826086956522\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(f'accuracy_score : {accuracy_score(rf_pred, y_test)}')\n",
    "print(f'precision_score : {precision_score(rf_pred, y_test)}')\n",
    "print(f'recall_score : {recall_score(rf_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "* random_state = 2022\n",
    "* 변수명 gb_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boost\n",
    "gb_clf = GradientBoostingClassifier(random_state = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=2022)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "gb_pred = gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score : 0.9108910891089109\n",
      "precision_score : 0.8823529411764706\n",
      "recall_score : 0.9375\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(f'accuracy_score : {accuracy_score(gb_pred, y_test)}')\n",
    "print(f'precision_score : {precision_score(gb_pred, y_test)}')\n",
    "print(f'recall_score : {recall_score(gb_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보고서 과제\n",
    "1. voting, bagging, random forest, boosting, adaboost, gradient boost 의 특징 및 장단점을 스스로 정리해보기\n",
    "2. Boosting의 advanced model 인 XGBoost, LightGBM, CatBoost에 대해 찾아보고 정리해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting\n",
    "다수의 classifier를 사용하여 투표를 통해 최종 예측 결과를 결정한다. 서로 다른 알고리즘을 가진 분류기를 같은 데이터 셋에 대하여 사용하며, hard voting은 다수의 classifier의 예측 결과값을 다수결을 통하여 최종 예측 결과를 결정하고, soft voting은 다수의 classifier의 예측 결과값간 확률의 평균을 통하여 최종 예측 결과를 결정한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging\n",
    "다수의 classifier를 사용하여 최종 결과를 예측한다. voting과 다르게 같은 알고리즘 유형을 가진 분류기가 서로 다른 데이터 셋을 대하여 모델을 학습한다. 각각의 예측 결과값간 확률의 평균을 통하여 최종 예측 결과를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest\n",
    "여러 개의 decision tree를 사용하여 최종 결과를 예측하는 방법이다. 기존의 decision tree는 train dataset에 대하여 쉽게 overfitting 될 수 있지만, 여러 개의 decision tree를 사용하여 여러 개의 예측 값의 확률값을 평균 내어 계산하는 과정을 통하여 일반화된 decision tree를 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boosting (adaboost, gradient boost)\n",
    "boosting은 초기 샘플 데이터를 뽑아내고 다수의 분류기를 생성한다는 것에서 bagging과 유사하다. bagging은 각각의 classifier와 data에 대하여 각각의 예측 결과를 내지만 boosting은 각각의 classifier가 서로에게 가중치를 매겨주는 방법을 택한다. 즉, weak classifier들을 각각의 weight를 통해 strong classifier를 만들어 최종 예측을 하는 방법이다.\n",
    "\n",
    "adaboost는 adaptive boosting의 약자로, 이전 분류기가 틀린 부분을 adaptive하게 바꾸어가며 잘못 분류되는 데이터에 집중하도록 한다. 예를 들어 첫번째 분류기에서 잘 못 분류된 것에 더 큰 가중치를 주어서 다음 분류기에서는 그 부분을 더 신경써서 분류하는 방법으로 진행한다.\n",
    "\n",
    "gradient boost는 가중치를 부여하는 방법에서 gradient descent를 사용한다. classifier의 결과값의 loss값을 weight에 대하여 편미분하여 gradient를 계산한 후, 그에 대해서 weight를 update한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost, LightGBM, CatBoost\n",
    "XGBoost는 gradient boosting을 발전시킨 모형이다. gradient boosting에서 optimization과 algorithmic에서 차이가 있다. 우선 parrlelization을 사용한다. 예를 들어 2중 loop 문에서 inner loop을 실행해야 outer loop가 돌아가는 형식을 병렬처리를 통해서 더 빠른 실행 속도를 가능하게 한다. 또한 max_depth를 결정하고 tree를 생성하여 계산상의 이득이 더 많다. 또한 regularization을 통하여 oveffitting을 방지한다.\n",
    "\n",
    "LightGBM은 기존의 level-wise하게 tree를 키워왔던 방법을 leaf-wise하게 tree를 키우는 방법을 선택했다. 트리의 균형을 맞추지 않고, leaf node를 지속적으로 분할하면서 진행한다. 이는 적은 데이터에 대해 overfitting이 야기될 수 있지만, 더 낮은 loss를 반환한다는 것이 장점이다.\n",
    "\n",
    "Catboost는 Categorical Boosting의 약자로 Categorical feature를 처리하는데 중점을 둔 알고리즘이다. CatBoost는 XGBoost와 같이 levelwise하게 tree를 크게 하지만 트리가 나누어지는 feature들이 대칭이라는 특징을 갖고 있다. 데이터를 한개씩 추가시키며 새로운 데이터에 대한 잔차를 계산하여 이를 이용하여 모델을 만드는 과정을 반복한다. 이로써 연산과정이 길어지지만 과적합 문제를 예방하는데 도움을 준다."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
