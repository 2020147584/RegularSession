{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data = np.loadtxt('data.csv', delimiter=',', dtype=float)\n",
    "labels = data[:, 0]\n",
    "features = preprocessing.minmax_scale(data[:, 1:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels.ravel(), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무\n",
    "* random_state = 2022 으로 설정\n",
    "* 변수명은 dt_clf 로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 부분 코드 작성\n",
    "# decision tree\n",
    "dt_clf =DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "dt_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "dt_pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811881188118812\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(dt_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트\n",
    "* random_state = 2022\n",
    "* 변수명 rf_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "rf_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504950495049505\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(rf_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "* random_state = 2022\n",
    "* 변수명 gb_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boost\n",
    "gb_clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "gb_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "gb_pred =gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9603960396039604\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(gb_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보고서 과제\n",
    "1. voting, bagging, random forest, boosting, adaboost, gradient boost 의 특징 및 장단점을 스스로 정리해보기\n",
    "2. Boosting의 advanced model 인 XGBoost, LightGBM, CatBoost에 대해 찾아보고 정리해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/10 보고서 과제 -권용호\n",
    "\n",
    "\n",
    "1. voting, bagging, random forest, boosting, adaboost, gradient boost 의 특징 및 장단점을 스스로 정리해보기\n",
    "\n",
    "- 앙상블 학습의 유형은 Voting, Bagging, Boosting 세가지로 나눌 수 있다. Voting과 Bagging은 여러개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식이다. Voting과 Bagging의 다른 점은 Voting의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합하는 것이고 배깅의 경우 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 Voting을 수행하는 것이다. 대표적인 Bagging방식이 Random forest 알고리즘이다. Boosting은 여러 개의 분류기가 순차적으로 학습을 수행하되 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 부여하면서 학습과 예측을 진행하는 것이다. 계속해서 분류기에게 가중치를 부스팅하면서 학습을 진행하기에 부스팅 방식이라고 부른다. AdaBoost는 이전의 분류기에 의해 잘못 분류된 것들을 이어지는 약한 학습기들이 수정해줄 수 있다는 점에서 다양한 상황에 적용할 수 있다. 따라서 AdaBoost는 잡음이 많은 데이터와 outlier에 취약한 모습을 보인다. 그러나 또 다른 경우에는, 다른 학습 알고리즘보다 overfitting에 덜 취약한 모습을 보이기도 한다. GBM(Gradient Boost Machine)도 AdaBoost와 유사하게 동작하지만 가중치 업데이트의 방식이 경사 하강법(Gradient descent)을 사용하는 것이 차이점이다. 또한, 어떠한 loss를 사용하든 이 loss function의 negative gradient를 다음 학습의 정답으로 사용하는 것이 GBM이다. 단지, 위에서 언급한 회귀문제에서는 mse를 loss function으로 사용했기에 이 loss function의 negative gadient인 residual을 학습한 것이다. gradient boosting은 ML 계열의 모델 중 성능이 좋은 편이라는 장점이 있지만 수행시간이 길다는 단점이 있다.\n",
    "\n",
    "\n",
    "\n",
    "2. Boosting의 advanced model 인 XGBoost, LightGBM, CatBoost에 대해 찾아보고 정리해보기\n",
    "\n",
    "XGBoost\n",
    "XGBoost는 Extreme Gradient Boosting의 약자이다. Boosting 기법을 이용하여 구현한 알고리즘은 Gradient Boost 가 대표적인데 이 알고리즘을 병렬 학습이 지원되도록 구현한 라이브러리가 XGBoost 이다. Regression, Classification 문제를 모두 지원하며, 성능과 자원 효율이 좋아서, 인기 있게 사용되는 알고리즘이다. 병렬 처리로 학습, 분류 속도가 빠르고  XGBoost는 자체에 과적합 규제 기능으로 강한 내구성 지닌다는 장점이 있다. 또한, 분류와 회귀영역에서 뛰어난 예측 성능 발휘한다. \n",
    "\n",
    "LightGBM\n",
    "일반적인 GBM 계열의 트리 분할 방식은 균형 트리 분할(Level Wise) 방식을 이용한다. 이 방법을 이용하면 최대한 균형 잡힌 트리를 생성하면서도 트리의 깊이를 최소화 할 수 있다는 장점이 있다. 이러한 방식으로 트리를 생성할 경우 과적합 문제에 더 강하 구조를 갖게 되지만 균형을 맞추기 위한 시간이 필요하다는 단점이 있다. 또한, 일반적인 균형 트리 분할 방식과 다르게 리프 중심 트리 분할(Leaf Wise) 방식을 이용한다. 이 방법은 트리의 균형을 맞추지 않고 최대 손실 값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리가 깊어지고 비대칭 적인 트리를 만든다. LightGBM과 XGBoost를 비교했을 때 아래와 같은 장점이 있다.\n",
    "빠른 학습 및 예측 수행 시간\n",
    "더 작은 메모리 사용량\n",
    "카테코리형 피처의 자동 변환 및 최적 분할\n",
    "\n",
    "\n",
    "CatBoost\n",
    "GBM의 치명적인 문제점 중 하나는 과적합 문제이다. CatBoost는 이 과적합 문제를 해결하면서 동시에 기존 GBM 계열 알고리즘인 XGBoost, LightGBM 알고리즘보다 학습 속도를 개선하는 장점을 앞세워 개발되었다. 또한 XGBoost, LightGBM이 Hyper-parameter에 따라 성능이 달라지는 민감한 문제를 해결하는 것에도 초점을 맞추었다. LightGBM은 알고리즘 종류 중 DFS(깊이 우선 탐색) 처럼 트리를 우선적으로 깊게 형성하는 방식을 취한다. 반면에 XGBoost는 BFS(너비 우선 탐색)처럼 우선적으로 넓게 트리를 형성하게 된다. 이 때 XGBoost 와 CatBoost와 차이점이 없는 것처럼 보이지만 자세히 보면 트리가 나누어지는 Feature들이 대칭인지 여부에 따라 차이점이 드러난다. CatBoost는 이렇게 Feature를 모두 동일하게 대칭적인 트리 구조를 형성하게 된다. 겉으로 보기에 이러한 대칭 트리 형성 구조가 비합리적이라고 보일 수 있지만 이는 예측 시간을 감소시킴으로써 기존 Boosting 계열 알고리즘이 느린 학습속도라는 측면에서는 CatBoost 만의 장점이 된다.Sparse Matrix 즉, 결측치가 매우 많은 데이터셋에는 부적합한 모델이다. 예를 들어, 추천시스템에 자주 사용되는 사용자-아이템 행렬 데이터를 살펴보면 보통 Sparse한 형태로 이루어져 있다. 만약 이러한 데이터를 활용하려면 Sparse한 특성이 없도록 Embedding을 적용한다던지 등 데이터를 변형한 후 CatBoost에 활용하는 것이 적합할 것이다. 또한 애초에 수치형 변수가 매우 많은 데이터라면 LightGBM 보다 학습 속도가 오래 걸린다"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
