{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "data = np.loadtxt('data.csv', delimiter=',', dtype=float)\n",
    "labels = data[:, 0]\n",
    "features = preprocessing.minmax_scale(data[:, 1:])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels.ravel(), test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무\n",
    "* random_state = 2022 으로 설정\n",
    "* 변수명은 dt_clf 로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 부분 코드 작성\n",
    "# decision tree\n",
    "dt_clf = DecisionTreeClassifier(splitter='random', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "dt_clf = dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "dt_pred = dt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9207920792079208\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트\n",
    "* random_state = 2022\n",
    "* 변수명 rf_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_clf = RandomForestClassifier(random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "rf_clf = rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "rf_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504950495049505\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost\n",
    "* random_state = 2022\n",
    "* 변수명 gb_clf 로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boost\n",
    "gb_clf = GradientBoostingClassifier(random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 분류기에 train set 피팅\n",
    "gb_clf = gb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋으로 prediction\n",
    "gb_pred = gb_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504950495049505\n"
     ]
    }
   ],
   "source": [
    "# 성능 확인\n",
    "# accuracy_score 계산\n",
    "print(accuracy_score(y_test, gb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보고서 과제\n",
    "1. voting, bagging, random forest, boosting, adaboost, gradient boost 의 특징 및 장단점을 스스로 정리해보기\n",
    "2. Boosting의 advanced model 인 XGBoost, LightGBM, CatBoost에 대해 찾아보고 정리해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. voting : 여러 분류기가 투표를 통해 결정 : 단점 : sample의 갯수가 많은 경우\n",
    "2. Bagging : 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과물을 집계 : 장점 : sample의 갯수가 많은 경우, 단점 : sample을 뽑는 방법 중요\n",
    "3. random forest : 의사 결정 나무를 이용한 대표적인 bagging 방식 : 장점 : 앙상블의 다양성 획득, 변수의 임의 추출, 다른 앙상블 보델에 비해 해석력이 높음, 단점 : tree의 다양성을 확보해야함\n",
    "4. Boosting : 여러 분류기가 순차적으로 학습 수행, 이전 학습에서 잘못 예측된 데이터에 가중치 부여 & 오차 보완\n",
    "5. adaboost : 가중치를 부여한 약 분류기를 모아 강 분류기를 만듬, 장점 : 랜덤 포레스트와 달리 특정 분류기가 다른 분류기보다 더 중요한 것을 고려\n",
    "6. Gradient boost : 5번이랑 비슷 but 잔차를 줄여나가는 방식 : 단점 : overfitting이 일어날 수 있는 문제가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBOOST<br>\n",
    "XGBOOST는 Gradient boosting 알고리즘의 단점을 보완해주기 위해 만들어진 알고리\n",
    "즘 입니다. 기존에 배운 Gradient boosting은 느리고, 과적합 이슈가 있다는 단점이\n",
    "있습니다.\n",
    "이를 해결하기 위해 Gradient Boosting을 조금 더 향상시킨 XGBOOST라는 모델을 만\n",
    "들게 되었는데 대표적인 장점들을 설명해 드리겠습니다.\n",
    "첫번째로 Gradient boosting model 보다 빠르다는 장점이 있는데 이는 Parallelized\n",
    "implementation을 통해 base learner를 만드는 loop의 시간을 단축시켜준다는 점이\n",
    "긍정적인 영향을 끼칩니다. 그리고 가지치기에서 max_depth까지 트리를 만든다음 밑\n",
    "부터 위로 올라가는 형식으로 split을 진행한다는 점 또한 빠른 알고리즘 실행이 가\n",
    "능토록 해줍니다.\n",
    "두번째로는 과적합 이슈를 해결했다는 점입니다. 기존의 Gradient boosting 알고리과\n",
    "달리 Regularization Term을 둠으로서 과적합을 방지할 수 있습니다.<br>\n",
    "\n",
    "- LIGHTGBM<br>\n",
    "LIGHTGBM은 위에서 말씀드렸던 XGBOOST와 비슷하게 기존 Gradient boosting 알고\n",
    "리즘을 발전시킨 알고리즘 입니다. LIGHTBOOST는 XGBOOST와 기존 Gradient\n",
    "boosting에 비해 속도가 빠르다는 장점이 있습니다.\n",
    "속도가 빨라지게 된 방법으로 Leaf-wise 즉 리프 중심 분할이라는 개념이 있습니다.\n",
    "앞서 말한 XGBOOST와 Gradient boosting 알고리즘은 트리를 만들 때 균형 트리 분\n",
    "할을 통해 최대한 균형 잡힌 트리를 만듭니다. 이를 통해 max_depth가 작은 트리가\n",
    "만들어지고 과적합을 방지할 수 있지만 균형을 맞추는 시간이 오래 걸린다는 단점이\n",
    "있습니다. 그러나 LIGHTGBM은 균형을 맞추지 않고 최대 손실 값을 가지는 리프노드\n",
    "를 지속적으로 분할하면서 트리의 깊이가 깊어지고, 비대칭적인 트리가 생성됩니다.\n",
    "이런 최대 손실 값을 가지는 리프를 계속 분할함으로서 예측 오류 손실을 최소화 시\n",
    "키면서 과적합이 방지 되게 되는데 단점이 한가지 있습니다. 바로 데이터 수가 적은\n",
    "경우 과적합이 일어날 수 있다는 점입니다.\n",
    "<br>\n",
    "- CATBOOST<br>\n",
    "Gradient boost는 차에만 포커스를 맞추어 학습하는 것이 매우 이상적이라고 보일지도 모르겠지만 모델이 본적 없는 데이터에는 예측을 잘 하지 못하는 과적합(Overfitting) 문제를 유발할 가능성이 매우 높다는 것이 치명적인 단점\n",
    "방법 : 주어진 전체 데이터를 임의적으로 N 개의 Fold로 나누어서 각 Fold에 속한 데이터셋들에 Ordered Boosting을 적용\n",
    "담점 : sparse matrix 즉 결측치가 매우 많은 데이터셋에는 부적합한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
